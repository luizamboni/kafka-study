version: '3.3'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.0.1
    hostname: zookeeper
    container_name: zookeeper
    environment:
      ALLOW_ANONYMOUS_LOGIN: 'yes'
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_SYNC_LIMIT: 2
    restart: unless-stopped

  kafka:
    image: confluentinc/cp-kafka:7.0.1
    hostname: broker
    container_name: broker
    ports:
      - 9092:9092
      - 9094:9094
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: INTERNAL://broker:9092,OUTSIDE://0.0.0.0:9094
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://broker:9092,OUTSIDE://${MACHINE_IP:-127.0.0.1}:9094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      ALLOW_PLAINTEXT_LISTENER: 'yes'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
    restart: unless-stopped

  schema-registry:
    image: confluentinc/cp-schema-registry:7.0.1
    hostname: schema-registry
    container_name: schema-registry
    depends_on:
      - kafka
      - zookeeper
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'broker:9092'
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
    restart: unless-stopped

  kafka-clients:
    build: 
      context: ./apps
      dockerfile: Dockerfile  
    environment:
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION}
      AWS_SESSION_TOKEN: ${AWS_SESSION_TOKEN}
    volumes:
      - ./apps/:/home/apps/

  connect:
    build: 
      context: ./connect
      dockerfile: Dockerfile
    # image: confluentinc/cp-kafka-connect-base:7.0.1
    container_name: kafka-connect
    ports:
      - 8083:8083
    environment:
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION}
      CONNECT_BOOTSTRAP_SERVERS: ${BROKER_ENDPOINTS}
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: kafka-connect
      CONNECT_CONFIG_STORAGE_TOPIC: _connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: _connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: _connect-status
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: ${CONFLUENT_REGISTRY}
      CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect"
      CONNECT_LOG4J_APPENDER_STDOUT_LAYOUT_CONVERSIONPATTERN: "[%d] %p %X{connector.context}%m (%c:%L)%n"
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: "-1"
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: "-1"
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: "-1"
    #  ---------------
      # CONNECT_PLUGIN_PATH: /usr/share/confluent-hub-components,/usr/share/filestream-connectors,/data/connect-jars
      CONNECT_PLUGIN_PATH: /usr/share/java,/usr/share/java/kafka,/usr/local/share/kafka/plugins,/usr/share/filestream-connectors,/data/connect-jars,/usr/share/confluent-hub-components

    # If you want to use the Confluent Hub installer to d/l component, but make them available
    # when running this offline, spin up the stack once and then run : 
    #   docker cp kafka-connect:/usr/share/confluent-hub-components ./data/connect-jars
    volumes:
      - $PWD/data:/data
    # In the command section, $ are replaced with $$ to avoid the error 'Invalid interpolation format for "command" option'
    command:
      - bash
      - -c
      - |
        # 
        cat <<EOT >> distributed.properties
        bootstrap.servers=${BROKER_ENDPOINTS}
        plugin.path=/usr/share/java,/usr/share/java/kafka,/usr/local/share/kafka/plugins,/usr/share/filestream-connectors,/data/connect-jars,/usr/share/confluent-hub-components

        value.converter=org.apache.kafka.connect.storage.StringConverter
        key.converter=org.apache.kafka.connect.json.JsonConverter
        group.id=abcd
        offset.storage.topic=_connect-offsets
        config.storage.topic=_connect-configs
        status.storage.topic=_connect-status
        
        offset.storage.replication.factor=-1
        config.storage.replication.factor=-1
        status.storage.replication.factor=-1
        tasks.max=1
        EOT

        cat <<EOT >> standalone.properties
        bootstrap.servers=${BROKER_ENDPOINTS}
        plugin.path=/usr/share/java,/usr/share/java/kafka,/usr/local/share/kafka/plugins,/usr/share/filestream-connectors,/data/connect-jars,/usr/share/confluent-hub-components

        value.converter=org.apache.kafka.connect.storage.StringConverter
        key.converter=org.apache.kafka.connect.json.JsonConverter
        group.id=abcd
        tasks.max=1
        
        offset.storage.file.filename=offset-file1
        EOT

        #
        cat <<EOT >> connector.properties
        name=${CONNECTOR_NAME}
        connector.class=io.confluent.connect.s3.S3SinkConnector
        tasks.max=1
        topics=${TOPIC_GENERIC_JSON}
        s3.region=us-east-1
        s3.bucket.name=confluent-kafka-connect-s3-study
        s3.part.size=5242880
        flush.size=3

        # transforms=SetSchema
        # transforms.SetSchema.type=org.apache.kafka.connect.transforms.SetSchema
        # transforms=SetSchema,SetSchemaMetadata


        key.converter=org.apache.kafka.connect.storage.StringConverter

        value.converter=io.confluent.connect.json.JsonSchemaConverter
        value.converter.schema.registry.url=${CONFLUENT_REGISTRY}

        storage.class=io.confluent.connect.s3.storage.S3Storage
        format.class=io.confluent.connect.s3.format.parquet.ParquetFormat
        schema.generator.class=io.confluent.connect.storage.hive.schema.DefaultSchemaGenerator
        partitioner.class=io.confluent.connect.storage.partitioner.HourlyPartitioner
        schema.compatibility=NONE
        locale=pt_BR
        timezone=UTC
        timestamp.extractor=Record
        EOT
        #
        echo "distributed.properties"
        cat distributed.properties
        echo "-------------------------"
        echo "connector.properties"
        cat connector.properties

        echo "Launching Kafka Connect worker"
        /bin/connect-distributed distributed.properties

        # /bin/connect-standalone standalone.properties connector.properties

  pg:
    image: postgres:14.2
    environment:
      POSTGRES_USER: pg-example
      POSTGRES_PASSWORD: pg-example
      POSTGRES_DB: pg-example

  ksqldb-server:
    image: confluentinc/cp-ksqldb-server:7.0.1
    hostname: ksqldb-server
    container_name: ksqldb-server
    depends_on:
      - kafka
      - connect
      - schema-registry
    ports:
      - "8088:8088"
    environment:
      KSQL_CONFIG_DIR: "/etc/ksql"
      KSQL_BOOTSTRAP_SERVERS: "broker:9092"
      KSQL_HOST_NAME: ksqldb-server
      KSQL_LISTENERS: "http://0.0.0.0:8088"
      KSQL_CACHE_MAX_BYTES_BUFFERING: 0
      KSQL_KSQL_SCHEMA_REGISTRY_URL: "http://schema-registry:8081"
      KSQL_PRODUCER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor"
      KSQL_CONSUMER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor"
      KSQL_KSQL_CONNECT_URL: "http://connect:8083"
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_REPLICATION_FACTOR: 1
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: 'true'
      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: 'true'
      # KSQL_KSQL_QUERIES_FILE: /ksql.sql
    restart: unless-stopped
    # volumes:
    #   - ./ksql/ksql.sql:/ksql.sql

  ksqldb-cli:
    image: confluentinc/cp-ksqldb-cli:7.0.1
    container_name: ksqldb-cli
    depends_on:
      - kafka
      - connect
      - ksqldb-server
    entrypoint: /bin/sh
    tty: true

  akhq: 
    image: tchiotludo/akhq
    ports: 
      - 8080:8080
    environment:
      AKHQ_CONFIGURATION: |
        akhq:
          connections:
            docker-kafka-server:
              properties:
                security.protocol: ${SECURITY_PROTOCOL}
                bootstrap.servers: ${BROKER_ENDPOINTS}
    restart: unless-stopped

  kafdrop:
    image: obsidiandynamics/kafdrop:3.27.0
    ports:
      - 9000:9000
    environment:
      KAFDROP_KAFKA_PROPERTIES: ${KAFDROP_KAFKA_PROPERTIES}
      KAFKA_BROKERCONNECT: ${BROKER_ENDPOINTS}
    restart: unless-stopped